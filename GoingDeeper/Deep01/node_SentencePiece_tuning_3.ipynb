{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0860fbd8",
   "metadata": {},
   "source": [
    " ### 4-5. SentencePiece í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë° ì„±ëŠ¥ ê°œì„  ì‹¤í—˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a9a480",
   "metadata": {},
   "source": [
    "#### ë°ì´í„° & SentencePiece ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41d579d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id                                           document  label\n",
      "0   9976970                                ì•„ ë”ë¹™.. ì§„ì§œ ì§œì¦ë‚˜ë„¤ìš” ëª©ì†Œë¦¬      0\n",
      "1   3819312                  í ...í¬ìŠ¤í„°ë³´ê³  ì´ˆë”©ì˜í™”ì¤„....ì˜¤ë²„ì—°ê¸°ì¡°ì°¨ ê°€ë³ì§€ ì•Šêµ¬ë‚˜      1\n",
      "2  10265843                                  ë„ˆë¬´ì¬ë°“ì—ˆë‹¤ê·¸ë˜ì„œë³´ëŠ”ê²ƒì„ì¶”ì²œí•œë‹¤      0\n",
      "3   9045019                      êµë„ì†Œ ì´ì•¼ê¸°êµ¬ë¨¼ ..ì†”ì§íˆ ì¬ë¯¸ëŠ” ì—†ë‹¤..í‰ì  ì¡°ì •      0\n",
      "4   6483659  ì‚¬ì´ëª¬í˜ê·¸ì˜ ìµì‚´ìŠ¤ëŸ° ì—°ê¸°ê°€ ë‹ë³´ì˜€ë˜ ì˜í™”!ìŠ¤íŒŒì´ë”ë§¨ì—ì„œ ëŠ™ì–´ë³´ì´ê¸°ë§Œ í–ˆë˜ ì»¤ìŠ¤í‹´ ...      1\n",
      "        id                                           document  label\n",
      "0  6270596                                                êµ³ ã…‹      1\n",
      "1  9274899                               GDNTOPCLASSINTHECLUB      0\n",
      "2  8544678             ë­ì•¼ ì´ í‰ì ë“¤ì€.... ë‚˜ì˜ì§„ ì•Šì§€ë§Œ 10ì  ì§œë¦¬ëŠ” ë”ë”ìš± ì•„ë‹ˆì–ì•„      0\n",
      "3  6825595                   ì§€ë£¨í•˜ì§€ëŠ” ì•Šì€ë° ì™„ì „ ë§‰ì¥ì„... ëˆì£¼ê³  ë³´ê¸°ì—ëŠ”....      0\n",
      "4  6723715  3Dë§Œ ì•„ë‹ˆì—ˆì–´ë„ ë³„ ë‹¤ì„¯ ê°œ ì¤¬ì„í…ë°.. ì™œ 3Dë¡œ ë‚˜ì™€ì„œ ì œ ì‹¬ê¸°ë¥¼ ë¶ˆí¸í•˜ê²Œ í•˜ì£ ??      0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "train_file = os.getenv('HOME') + '/aiffel/sp_tokenizer/naver_data/ratings_train.txt'\n",
    "test_file = os.getenv('HOME') + '/aiffel/sp_tokenizer/naver_data/ratings_test.txt'\n",
    "\n",
    "# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "train_data = pd.read_csv(train_file, sep='\\t')\n",
    "test_data = pd.read_csv(test_file, sep='\\t')\n",
    "\n",
    "# ë°ì´í„° í™•ì¸\n",
    "print(train_data.head())\n",
    "print(test_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ccd49db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì „ì²˜ë¦¬ í›„ í•™ìŠµ ë°ì´í„° ê°œìˆ˜: 146183\n",
      "ì „ì²˜ë¦¬ í›„ í…ŒìŠ¤íŠ¸ ë°ì´í„° ê°œìˆ˜: 49158\n"
     ]
    }
   ],
   "source": [
    "# ì¤‘ë³µ ì œê±°\n",
    "train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "test_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "\n",
    "# íŠ¹ìˆ˜ ë¬¸ì ì œê±° (ì •ê·œ í‘œí˜„ì‹ ì‚¬ìš©)\n",
    "train_data['document'] = train_data['document'].str.replace(\"[^ã„±-ã…ã…-ã…£ê°€-í£ ]\", \"\", regex=True)\n",
    "test_data['document'] = test_data['document'].str.replace(\"[^ã„±-ã…ã…-ã…£ê°€-í£ ]\", \"\", regex=True)\n",
    "\n",
    "print(f\"ì „ì²˜ë¦¬ í›„ í•™ìŠµ ë°ì´í„° ê°œìˆ˜: {len(train_data)}\")\n",
    "print(f\"ì „ì²˜ë¦¬ í›„ í…ŒìŠ¤íŠ¸ ë°ì´í„° ê°œìˆ˜: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ee185da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í•„í„°ë§ í›„ í•™ìŠµ ë°ì´í„° ê°œìˆ˜: 145791\n",
      "í•„í„°ë§ í›„ í…ŒìŠ¤íŠ¸ ë°ì´í„° ê°œìˆ˜: 48995\n"
     ]
    }
   ],
   "source": [
    "# ê²°ì¸¡ê°’(NaN) ì œê±°\n",
    "train_data = train_data.dropna(subset=['document']).copy()\n",
    "test_data = test_data.dropna(subset=['document']).copy()\n",
    "\n",
    "# ë¬¸ì¥ ê¸¸ì´ í•„í„°ë§ (NaN ë°©ì§€)\n",
    "filtered_train_data = train_data[train_data['document'].apply(lambda x: 1 <= len(str(x)) <= 140)].copy()\n",
    "filtered_test_data = test_data[test_data['document'].apply(lambda x: 1 <= len(str(x)) <= 140)].copy()\n",
    "\n",
    "print(f\"í•„í„°ë§ í›„ í•™ìŠµ ë°ì´í„° ê°œìˆ˜: {len(filtered_train_data)}\")\n",
    "print(f\"í•„í„°ë§ í›„ í…ŒìŠ¤íŠ¸ ë°ì´í„° ê°œìˆ˜: {len(filtered_test_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33cf2b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì•„ ë”ë¹™ ì§„ì§œ ì§œì¦ë‚˜ë„¤ìš” ëª©ì†Œë¦¬\r\n",
      "í í¬ìŠ¤í„°ë³´ê³  ì´ˆë”©ì˜í™”ì¤„ì˜¤ë²„ì—°ê¸°ì¡°ì°¨ ê°€ë³ì§€ ì•Šêµ¬ë‚˜\r\n",
      "ë„ˆë¬´ì¬ë°“ì—ˆë‹¤ê·¸ë˜ì„œë³´ëŠ”ê²ƒì„ì¶”ì²œí•œë‹¤\r\n",
      "êµë„ì†Œ ì´ì•¼ê¸°êµ¬ë¨¼ ì†”ì§íˆ ì¬ë¯¸ëŠ” ì—†ë‹¤í‰ì  ì¡°ì •\r\n",
      "ì‚¬ì´ëª¬í˜ê·¸ì˜ ìµì‚´ìŠ¤ëŸ° ì—°ê¸°ê°€ ë‹ë³´ì˜€ë˜ ì˜í™”ìŠ¤íŒŒì´ë”ë§¨ì—ì„œ ëŠ™ì–´ë³´ì´ê¸°ë§Œ í–ˆë˜ ì»¤ìŠ¤í‹´ ë˜ìŠ¤íŠ¸ê°€ ë„ˆë¬´ë‚˜ë„ ì´ë»ë³´ì˜€ë‹¤\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 5 $HOME/aiffel/sp_tokenizer/data/nsmc_corpus.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64a9dd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 369K Feb 25 05:42 /aiffel/aiffel/sp_tokenizer/data/nsmc_spm.model\r\n",
      "-rw-r--r-- 1 root root 144K Feb 25 05:42 /aiffel/aiffel/sp_tokenizer/data/nsmc_spm.vocab\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh $HOME/aiffel/sp_tokenizer/data/nsmc_spm*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14de936",
   "metadata": {},
   "source": [
    "#### SentencePiece ëª¨ë¸ ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd458b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í† í° ID: [19, 5, 21, 1144, 0]\n",
      "í† í°í™”ëœ ë¬¸ì¥: ['â–ì´', 'â–ì˜í™”', 'â–ì •ë§', 'â–ì¬ë¯¸ìˆë‹¤', '!']\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "import os\n",
    "\n",
    "# í•™ìŠµëœ SentencePiece ëª¨ë¸ ë¡œë“œ\n",
    "model_path = os.getenv('HOME') + '/aiffel/sp_tokenizer/data/nsmc_spm.model'\n",
    "s = spm.SentencePieceProcessor()\n",
    "s.Load(model_path)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë¬¸ì¥\n",
    "test_sentence = \"ì´ ì˜í™” ì •ë§ ì¬ë¯¸ìˆë‹¤!\"\n",
    "print(\"í† í° ID:\", s.EncodeAsIds(test_sentence))\n",
    "print(\"í† í°í™”ëœ ë¬¸ì¥:\", s.EncodeAsPieces(test_sentence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834a663b",
   "metadata": {},
   "source": [
    "#### sp_tokenize() í•¨ìˆ˜ êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f324ef30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def sp_tokenize(s, corpus): \n",
    "    tensor = []\n",
    "\n",
    "    # ë¬¸ì¥ì„ í† í° ID ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "    for sen in corpus:\n",
    "        tensor.append(s.EncodeAsIds(sen))\n",
    "\n",
    "    # ë‹¨ì–´-ì¸ë±ìŠ¤ ë§¤í•‘ ìƒì„±\n",
    "    vocab_file = os.getenv('HOME') + '/aiffel/sp_tokenizer/data/nsmc_spm.vocab'\n",
    "\n",
    "    with open(vocab_file, 'r', encoding='utf-8') as f:\n",
    "        vocab = f.readlines()\n",
    "\n",
    "    word_index = {}\n",
    "    index_word = {}\n",
    "\n",
    "    for idx, line in enumerate(vocab):\n",
    "        word = line.split(\"\\t\")[0]  # ë‹¨ì–´ ì¶”ì¶œ\n",
    "\n",
    "        word_index.update({word: idx})  # ë‹¨ì–´ -> ì¸ë±ìŠ¤ ë§¤í•‘\n",
    "        index_word.update({idx: word})  # ì¸ë±ìŠ¤ -> ë‹¨ì–´ ë§¤í•‘\n",
    "\n",
    "    # íŒ¨ë”© ì ìš©\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "    return tensor, word_index, index_word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe623c90",
   "metadata": {},
   "source": [
    "#### sp_tokenize() í•¨ìˆ˜ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b88a744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Tensor: [[  19    5   21 1366    0]\n",
      " [ 116  383  871    0    0]\n",
      " [ 215  431 4729    0    0]]\n",
      "Word Index ì˜ˆì‹œ: [('<unk>', 0), ('<s>', 1), ('</s>', 2), ('â–', 3), ('ì´', 4), ('â–ì˜í™”', 5), ('ì˜', 6), ('ë„', 7), ('ê°€', 8), ('ëŠ”', 9)]\n",
      "Index Word ì˜ˆì‹œ: [(0, '<unk>'), (1, '<s>'), (2, '</s>'), (3, 'â–'), (4, 'ì´'), (5, 'â–ì˜í™”'), (6, 'ì˜'), (7, 'ë„'), (8, 'ê°€'), (9, 'ëŠ”')]\n"
     ]
    }
   ],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„\n",
    "sample_sentences = [\"ì´ ì˜í™” ì •ë§ ì¬ë¯¸ìˆì–´ìš”!\", \"ì™„ì „ ìµœì•…ì´ì•¼.\", \"ê¸°ëŒ€ ì´ìƒì´ì—ˆìŠµë‹ˆë‹¤.\"]\n",
    "\n",
    "# í† í°í™” ì‹¤í–‰\n",
    "tokenized_tensor, word_index, index_word = sp_tokenize(s, sample_sentences)\n",
    "\n",
    "# ê²°ê³¼ í™•ì¸\n",
    "print(\"Tokenized Tensor:\", tokenized_tensor)\n",
    "print(\"Word Index ì˜ˆì‹œ:\", list(word_index.items())[:10])  # ì¼ë¶€ ë‹¨ì–´ ì¶œë ¥\n",
    "print(\"Index Word ì˜ˆì‹œ:\", list(index_word.items())[:10])  # ì¼ë¶€ ë‹¨ì–´ ì¶œë ¥\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d107685d",
   "metadata": {},
   "source": [
    "#### ë„¤ì´ë²„ ì˜í™” ë¦¬ë·° ì „ì²´ ë°ì´í„°ì…‹ ë³€í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a0ae866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë³€í™˜ëœ í›ˆë ¨ ë°ì´í„° í¬ê¸°: (145791, 116)\n",
      "ë³€í™˜ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°: (48995, 107)\n"
     ]
    }
   ],
   "source": [
    "# í›ˆë ¨ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„° ë³€í™˜\n",
    "train_tensor, train_word_index, train_index_word = sp_tokenize(s, filtered_train_data['document'].tolist())\n",
    "test_tensor, test_word_index, test_index_word = sp_tokenize(s, filtered_test_data['document'].tolist())\n",
    "\n",
    "print(\"ë³€í™˜ëœ í›ˆë ¨ ë°ì´í„° í¬ê¸°:\", train_tensor.shape)\n",
    "print(\"ë³€í™˜ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°:\", test_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc45fe3",
   "metadata": {},
   "source": [
    "#### ê°ì„± ë¶„ì„ ëª¨ë¸ ì„¤ê³„\n",
    "##### Org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11633be9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 128)         1024000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,073,473\n",
      "Trainable params: 1,073,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "vocab_size = 8000  # SentencePieceì—ì„œ ì„¤ì •í•œ vocab_sizeì™€ ë™ì¼\n",
    "embedding_dim = 128\n",
    "hidden_units = 64 # ê°ì • ë¶„ì„ ìœ„í•´\n",
    "\n",
    "# ëª¨ë¸ ì„¤ê³„\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, mask_zero=True),\n",
    "    LSTM(hidden_units),\n",
    "    Dense(1, activation='sigmoid')  # ê°ì • ë¶„ì„ (ì´ì§„ ë¶„ë¥˜)\n",
    "])\n",
    "\n",
    "# ëª¨ë¸ ì»´íŒŒì¼\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# ëª¨ë¸ êµ¬ì¡° í™•ì¸\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88582fe9",
   "metadata": {},
   "source": [
    "#### embedding_dim ë³€ê²½ ì‹¤í—˜ (256, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e964d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ ì‹¤í—˜: embedding_dim = 256\n",
      "Epoch 1/5\n",
      "2278/2278 - 673s - loss: 0.3769 - accuracy: 0.8281 - val_loss: 0.3384 - val_accuracy: 0.8497\n",
      "Epoch 2/5\n"
     ]
    }
   ],
   "source": [
    "for embedding_dim in [256, 512]:\n",
    "    print(f\"ğŸ“Œ ì‹¤í—˜: embedding_dim = {embedding_dim}\")\n",
    "\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=8000, output_dim=embedding_dim, mask_zero=True),\n",
    "        LSTM(64),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # ë ˆì´ë¸” ë°ì´í„° ì¤€ë¹„\n",
    "    train_labels = filtered_train_data['label'].values\n",
    "    test_labels = filtered_test_data['label'].values\n",
    "\n",
    "    # ëª¨ë¸ í•™ìŠµ\n",
    "    model.fit(train_tensor, train_labels, validation_data=(test_tensor, test_labels),\n",
    "              epochs=5, batch_size=64, verbose=2)\n",
    "\n",
    "    # í…ŒìŠ¤íŠ¸ í‰ê°€\n",
    "    test_loss, test_acc = model.evaluate(test_tensor, test_labels)\n",
    "    print(f\"ğŸ“Š embedding_dim = {embedding_dim}, í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62877765",
   "metadata": {},
   "source": [
    "### íšŒê³ \n",
    "\n",
    "ìƒê°ë³´ë‹¤ ì „ì²´ì ìœ¼ë¡œ í•™ìŠµ ì‹œê°„ì´ ë‹¤ì†Œ ì†Œìš”ëë‹¤.   \n",
    "ê·¸ë˜ë„ KoNLPy ë¹„êµ ì‹¤í—˜ ê¹Œì§€ëŠ” ìˆ˜í–‰í–ˆëŠ”ë° Oktê°€ ìƒê°ë³´ë‹¤ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë ¤ì„œ ì‹¤í—˜ì—ì„œ ì œì™¸í–ˆë‹¤.   \n",
    "ë˜í•œ ëª¨ë¸ íŒŒë¼ë¯¸í„° ë³€ê²½ ì‹¤í—˜ë„ êµ‰ì¥íˆ ëŠë¦¬ê²Œ ì§„í–‰ ë˜ê³  ìˆë‹¤.   \n",
    "ì‚¬ì‹¤ ì‹œê°„ì´ ì¢€ ë” ì—¬ìœ ê°€ ìˆì—ˆìœ¼ë©´ vocab_size ë³€ê²½ ì‹¤í—˜ì€ ê¼­ í•´ë³´ê³  ì‹¶ì—ˆëŠ”ë° ì¶”í›„ ì—¬ìœ ê°€ ìˆì„ ë•Œ ìˆ˜í–‰í•˜ê³  ì‹¶ë‹¤.  \n",
    "ìš´ì´ ì¢‹ì€ ê±´ì§€ (?) ì²« ì‹œë„ì— ë°”ë¡œ SentencePiece RNN ëª¨ë¸ í…ŒìŠ¤íŠ¸ì—ì„œ ë°”ë¡œ ê²°ê³¼ê°€ 0.8ì´ ë„˜ì—ˆëŠ”ë° ì •í™•ë„ë¥¼ ë” ë†’ì¼ ìˆ˜ ìˆëŠ” ë°©ë²•ë„ ì¢€ ìƒê°í•´ë´ì•¼ê² ë‹¤.   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
